---
title: "statistics notebook"
output: html_notebook
---

# An ecologically-flavoured introduction to statistics - Day 1

## Goal
acquire an intuition about statistical methods: their nature, usage scenarios, and limitations

## literature
- Galileo Galilei - Il saggiatore
- Sally Calrdwell - Statisctics unplugged
- David Fredman, Robert Pisani, Roger Purves - Statistics
- Darrell Huff - How to lie with statistics
- Shannons theory of information (degree of surprise / contrast)

## Links

- [mathematic symbols 1](https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html)
- [mathematic symbols 2](https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols)
- [rmarkdown cheatsheet](https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

## Definitions
### Statistic
A numerical value which characterises the *sample* or *population* from which it was derived
$\rightarrow$ statistics is the study of the collection, analysis, interpretation, presentation, and organisation of data
### Population (sample set)
A complete set fo items that share at least one property in common that is the subject of a statistical analysis.
### Sample (event set)
A *limited number* of observations systematically or (pseudo)randomly selected from a population, whose analysis may yield generalisations about the source population. $\rightarrow$ must be representative
### Statistical estimator
The estimators estimate a population parameter based on sample data

- $\overline{x}$ estimates $\mu$
- $\mu$ is the population *mean*
- $\overline{x}$ is the sample *mean*
- a sample size of 3 is too small to calculate the mean

## Plan an experiment
Design your experiment with the data analysis in mind!

- minimum number if replicates needed $\rightarrow$ perform a power analysis
- disentangle the influence of confounding variables
- determine the statistical analyses that will be used
- one replicates a treatment/effect on an experimental unit
- the effect needs to be independet on one experimental unit
- replicates need to be independet on one another

## data types

- binary (logical)
- quantitative (discontinuous)
- quantitative (continuous)
- ordinal / ranked
- categorical / nominal (can be transformed with dummy numeric variables)

## Location of data
### Pythagorean means
- *the arithmetic mean*: $\overline{x}=\frac{1}{n}\sum_{i=1}^{n} x_{i}$
$\rightarrow$ often refered to as expected value E(X), where X is a random variable
- *the geometric mean*: $\sqrt[n]{\prod_{i=1}^{n} x_{i}}$
- *the harmonic mean*: $\overline{x}=n\sum_{i=1}^{n} \frac{1}{x_{i}}$
$\rightarrow$ inequality concerning AM, GM, and HM: $AM \ge GM \ge HM$

### Statistical location
- *the median*: order your data and take the middle value
- *the mode*: most commen value of a data series
$\rightarrow$ multiple modes possible: unimodal<bimodal<multimodal
$\rightarrow$ different distributions call for different measures of center! The arithmetic mean is not always valid!
- *skewness*: measure of symmetry
$\rightarrow$ negative (left) skewness: the mean of the data values is less than the median
$\rightarrow$ positive (right) skewness: the mean of the data values is larger than the median

## Spread of data

- *range*: highest - lowest value
- *sample variance*: $s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_{i}-\overline{x})^2$ 
$\rightarrow$ $s^2 \ge 0$
$\rightarrow$ $s^2 = 0$ all values are equal
- *standard deviation*: $s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_{i}-\overline{x})^2}$
$\rightarrow$ standardised form of the variance
- *coefficient of variation*: $\frac{s}{\overline{x}}$
$\rightarrow$ used when comparing multiple samples with different means. Standardisation!
- *concept of central moments*: variance, skewness, and kurtosis are all related through this concept

# An ecologically-flavoured introduction to statistics - Day 2

## The Random Variable

- $X \sim f(p_{1} \dots p_{n})$
- $(x_{i} \dots x_{n}) \in X$
- a variable whose value varies according to chance
- each value has an assigned probability

## Normal distribution

- $X \sim N(\mu, \sigma)$
- only controlled by two variables
- *probability density funciton*: $P(X) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
- Even if each sample-derived ramdom variable in a large collection of random variables (from the same population) is non-Gaussian, their means are distributed normally!
$\rightarrow$ *central limit theorem*

## Data transformation

- $y' = my+b$
- $y' = \sqrt{y}$
- $y' = \log{y}$
- *Z-score*: $Z = \frac{x_{i}-\overline{x}}{S_{x}}$
$\rightarrow$ standardization! without units!
$\rightarrow$ Based on the normal distribution, standardises normal data to zero mean and unit variance

## Various Distributions

### The Student's t distribution
The result of trying to estimate the mean of a normally distributed population when *n is small and $\sigma$ is unknown*

- n = ~ 30
- $X \sim f(p_{1} \dots p_{n})$
- $X \sim T(\nu)$
- $\nu > 0$: the degrees of freedom

### Degrees of freedom
The number of independent units of information used in the estimation of a parameter or calculation of a statistic ~ the numer of values that are 'free to vary' in this process.
n values have n degrees of freedom until something (e.g. a mean) is estimated - this consumes a degree of freedom $\rightarrow$ this does not apply to non-parametric tests (e.g. spearman correlation coefficient) because the tests use ranks.

### The Discrete Random Variable
A variable whose value, *which is always an integer*, varies according to 'chance'. Each possible value has a probability assigned to it.

- probability mass function for a discrete variable
- assigns a probability to a discrete outcome
- $\overline{x} \rightarrow \mu_{x} = \sum_{i=1}^N p_{i} x_{i}$
- $s^2 \rightarrow \sigma_{x}^2 = \sum_{i=1}^N p_{i}(x_{i}-\mu_{x})^2$

![](media/random_variable.jpg)

### The Binomial Distribution
The discrete probability distribution of the number of successes in n, independent yes/no experiments. Each trial is associated with a probability of success p, and a probability of failure q (complement), where $q = 1 -p$ (One of the popular discrete variables)

- $X \sim B(n,p)$
- $n \in N$: number of trials $\rightarrow$ not the number of samples from a B(n,p) distribution, but trials within the distribution
- $p \in [0,1]$: failure, success
- the number of successes k, after n independet trials is given by: $P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$ $\rightarrow$ 
- Bernoulli distribution: yes / no results

### The Poisson Distribution
Associated with rare events $\rightarrow$ probability of k events happening in a given time period or in a given spatial range where $\lambda$ is the expected number of events (known or estimated)
$\rightarrow$ $\approx B(n,p)$ with large n and small p

- $X \sim Pois(\lambda)$
- $\lambda > 0$: equivalent to E(X) and Var(X)
- for $k \in \{1,2,\dots,n\}$, the probability mass funtion of X is: $P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!}$

$\rightarrow$ discrete probability mass function

## Character

*Covariance*
$\rightarrow$ how do the variances of two variables relate?

*Correlation*
$\rightarrow$ Pearson's coefficient: only linear correlation
- Cov(X,Y)/sx*sy
- cares about values
$\rightarrow$ Spearman's rank coefficient: 
- gives the data ranks
- useful for ordinal data

*Linear Regression*
- y = mx + b
- x is the explainatory factor (independent variable)
- y is the response (dependent variable)
- m is the slope
- b is the intercept

- R2: How much covariation does this model explain
- residual: difference between predicted and observed values

- the model transforms the x variable to an approximation of y

# An ecologically-flavoured introduction to statistics - Day 3

## $H_{0}$: The null hypothesis
Good experimentalists attempt to falsify hypotheses

## Hypothesis testing
### General procedure

- clearly state $H_{0}, H_{A}, \alpha$ and n
- generate the null distribution
- collect data
-
-

### What is a statistically 'significant' result?
If we assume a certain sampling distribution holds for a given statistic, then we can predict how it 'should' ...

### The logic of the P value

### Confidence interval

### The one-sample t-test

### The two-sample t-test
- insterested in the location

### Z-test
- squares missing

### F-test
- interested in the shape

### Analysis of Variance (ANOVA)
- homoscedasticity
- heteroscedasticity: the variance increases with the x variable
- SSbetween --> fit
- SSwithin --> error

### The $\chi^2$

### Fisher's exact test

## Non-parametric tests
- more robust

## Resampling

### Jackknifing

### Bootstrapping

### Permutation
- within rows
- of rows
- within columns
- of columns





